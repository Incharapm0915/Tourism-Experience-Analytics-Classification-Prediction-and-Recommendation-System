{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28612f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOURISM ANALYTICS - DATA CLEANING AND PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "1. LOADING DATA WITH AUTO-DETECTION\n",
      "--------------------------------------------------\n",
      "Current working directory: d:\\Tourism-Analytics\\notebooks\n",
      "Using data path: ../data/raw/\n",
      "‚úì transactions: 52,930 rows\n",
      "‚úì users: 33,530 rows\n",
      "‚úì items: 1,698 rows\n",
      "‚úì cities: 9,143 rows\n",
      "‚úì countries: 165 rows\n",
      "‚úì regions: 22 rows\n",
      "‚úì continents: 6 rows\n",
      "‚úì attraction_types: 17 rows\n",
      "‚úì visit_modes: 6 rows\n",
      "\n",
      "Loaded 9 datasets successfully\n",
      "\n",
      "2. DATA QUALITY ASSESSMENT AND CLEANING\n",
      "--------------------------------------------------\n",
      "\n",
      "üìä CLEANING TRANSACTIONS\n",
      "----------------------------------------\n",
      "Shape change: (52930, 7) ‚Üí (14185, 7)\n",
      "Cleaning actions taken:\n",
      "  ‚Ä¢ Removed 38745 rows with invalid VisitYear values\n",
      "\n",
      "üìä CLEANING USERS\n",
      "----------------------------------------\n",
      "Missing values found:\n",
      "  CityId: 4 (0.0%)\n",
      "Shape change: (33530, 5) ‚Üí (33530, 5)\n",
      "Cleaning actions taken:\n",
      "  ‚Ä¢ Filled CityId missing values with median\n",
      "\n",
      "üìä CLEANING ITEMS\n",
      "----------------------------------------\n",
      "Shape change: (1698, 5) ‚Üí (1698, 5)\n",
      "No cleaning required - data was already clean!\n",
      "\n",
      "üìä CLEANING CITIES\n",
      "----------------------------------------\n",
      "Missing values found:\n",
      "  CityName: 1 (0.0%)\n",
      "Shape change: (9143, 3) ‚Üí (9143, 3)\n",
      "Cleaning actions taken:\n",
      "  ‚Ä¢ Filled CityName missing values with mode\n",
      "\n",
      "üìä CLEANING COUNTRIES\n",
      "----------------------------------------\n",
      "Shape change: (165, 3) ‚Üí (165, 3)\n",
      "No cleaning required - data was already clean!\n",
      "\n",
      "üìä CLEANING REGIONS\n",
      "----------------------------------------\n",
      "Shape change: (22, 3) ‚Üí (22, 3)\n",
      "No cleaning required - data was already clean!\n",
      "\n",
      "üìä CLEANING CONTINENTS\n",
      "----------------------------------------\n",
      "Shape change: (6, 2) ‚Üí (6, 2)\n",
      "No cleaning required - data was already clean!\n",
      "\n",
      "üìä CLEANING ATTRACTION_TYPES\n",
      "----------------------------------------\n",
      "Shape change: (17, 2) ‚Üí (17, 2)\n",
      "No cleaning required - data was already clean!\n",
      "\n",
      "üìä CLEANING VISIT_MODES\n",
      "----------------------------------------\n",
      "Shape change: (6, 2) ‚Üí (6, 2)\n",
      "No cleaning required - data was already clean!\n",
      "\n",
      "3. DATA INTEGRATION AND MASTER DATASET CREATION\n",
      "--------------------------------------------------\n",
      "Creating integrated master dataset...\n",
      "Starting with transactions: 14,185 records\n",
      "After joining users: 14,185 records\n",
      "After joining attractions: 14,185 records\n",
      "Joined cities: +2 columns\n",
      "Joined countries: +2 columns\n",
      "Joined regions: +2 columns\n",
      "Joined continents: +1 columns\n",
      "Joined attraction_types: +1 columns\n",
      "Joined visit_modes: +2 columns\n",
      "\n",
      "Final master dataset: 14,185 rows √ó 25 columns\n",
      "Columns: ['TransactionId', 'UserId', 'VisitYear', 'VisitMonth', 'VisitMode', 'AttractionId', 'Rating', 'ContinentId', 'RegionId', 'CountryId', 'CityId', 'AttractionCityId', 'AttractionTypeId', 'Attraction', 'AttractionAddress', 'CityName', 'CountryId_cities', 'Country', 'RegionId_countries', 'Region', 'ContinentId_regions', 'Continent', 'AttractionType', 'VisitModeId', 'VisitMode_visit_modes']\n",
      "\n",
      "Join Quality Assessment:\n",
      "  User Geography: 100.0% join success rate\n",
      "  Attraction Info: 100.0% join success rate\n",
      "\n",
      "4. FEATURE ENGINEERING\n",
      "--------------------------------------------------\n",
      "Creating derived features...\n",
      "‚úì Created temporal features (Season, MonthCategory)\n",
      "‚úì Created user aggregation features\n",
      "‚úì Created attraction aggregation features\n",
      "‚úì Created geographic diversity features\n",
      "‚úì Created rating deviation features\n",
      "‚úì Created visit mode preference features\n",
      "\n",
      "Feature engineering complete. Dataset now has 42 columns\n",
      "\n",
      "5. CATEGORICAL ENCODING\n",
      "--------------------------------------------------\n",
      "Categorical columns to encode: ['AttractionTypeId', 'Country', 'Region', 'Continent', 'AttractionType', 'VisitMode_visit_modes', 'Season', 'MonthCategory']\n",
      "‚úì Label encoded AttractionTypeId: 17 unique values\n",
      "‚úì Label encoded Country: 125 unique values\n",
      "‚úì Label encoded Region: 21 unique values\n",
      "‚úì Label encoded Continent: 5 unique values\n",
      "‚úì Label encoded AttractionType: 17 unique values\n",
      "‚úì Label encoded VisitMode_visit_modes: 5 unique values\n",
      "‚úì Label encoded Season: 4 unique values\n",
      "‚úì Label encoded MonthCategory: 2 unique values\n",
      "‚úì One-hot encoded: ['Season', 'MonthCategory']\n",
      "Dataset shape after encoding: (14185, 54)\n",
      "\n",
      "6. FINAL DATA QUALITY VALIDATION\n",
      "--------------------------------------------------\n",
      "‚úì No missing values in final dataset\n",
      "\n",
      "Data type summary:\n",
      "  int64: 18 columns\n",
      "  uint8: 10 columns\n",
      "  object: 9 columns\n",
      "  float64: 6 columns\n",
      "  bool: 6 columns\n",
      "  uint16: 3 columns\n",
      "  uint32: 2 columns\n",
      "\n",
      "Memory usage: 11.27 MB\n",
      "\n",
      "Key statistics:\n",
      "  Rating range: 1 - 5\n",
      "  Average rating: 4.16\n",
      "  Total records: 14,185\n",
      "\n",
      "7. DATA SPLITTING FOR MODELING\n",
      "--------------------------------------------------\n",
      "Regression split:\n",
      "  Training: (11348, 48)\n",
      "  Testing: (2837, 48)\n",
      "  Features: 48\n",
      "Classification split:\n",
      "  Training: (11348, 47)\n",
      "  Testing: (2837, 47)\n",
      "  Features: 47\n",
      "Recommendation matrix: (9688, 30)\n",
      "  Sparsity: 95.7%\n",
      "\n",
      "8. SAVING PROCESSED DATA\n",
      "--------------------------------------------------\n",
      "‚úì Saved master dataset: ../data/processed/master_dataset.csv\n",
      "‚úì Saved regression train/test splits\n",
      "‚úì Saved classification train/test splits\n",
      "‚úì Saved recommendation data\n",
      "‚úì Saved processing metadata\n",
      "\n",
      "9. DATA PREPROCESSING SUMMARY\n",
      "--------------------------------------------------\n",
      "‚úÖ DATA PREPROCESSING COMPLETED SUCCESSFULLY\n",
      "   Original data: 14,185 transactions\n",
      "   Final dataset: 14,185 rows √ó 54 columns\n",
      "   Memory usage: 11.27 MB\n",
      "   Data quality: ‚úì Excellent\n",
      "\n",
      "üìã Ready for modeling:\n",
      "   ‚Ä¢ Regression (Rating prediction): 48 features\n",
      "   ‚Ä¢ Classification (Visit mode): 47 features\n",
      "   ‚Ä¢ Recommendation system: User-item matrix ready\n",
      "\n",
      "üìÅ Processed files saved to: ../data/processed/\n",
      "   ‚Ä¢ master_dataset.csv\n",
      "   ‚Ä¢ train/test splits for regression and classification\n",
      "   ‚Ä¢ recommendation data and user-item matrix\n",
      "   ‚Ä¢ processing metadata\n",
      "\n",
      "================================================================================\n",
      "üéØ NEXT STEP: Feature Engineering and Model Building\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Tourism Experience Analytics - Data Cleaning and Preprocessing\n",
    "# Step 2: Data Cleaning, Integration, and Feature Engineering\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOURISM ANALYTICS - DATA CLEANING AND PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1: DATA LOADING WITH PATH AUTO-DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n1. LOADING DATA WITH AUTO-DETECTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Auto-detect correct path\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "possible_paths = ['data/raw/', '../data/raw/', '../../data/raw/', 'data\\\\raw\\\\', '..\\\\data\\\\raw\\\\']\n",
    "correct_path = None\n",
    "\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path + 'Transaction.xlsx'):\n",
    "        correct_path = path\n",
    "        break\n",
    "\n",
    "if correct_path is None:\n",
    "    print(\"ERROR: Cannot locate data files. Please check your directory structure.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Using data path: {correct_path}\")\n",
    "\n",
    "# Load all datasets\n",
    "datasets = {}\n",
    "file_paths = {\n",
    "    'transactions': 'Transaction.xlsx',\n",
    "    'users': 'User.xlsx',\n",
    "    'items': 'Updated_Item.xlsx',\n",
    "    'cities': 'City.xlsx',\n",
    "    'countries': 'Country.xlsx',\n",
    "    'regions': 'Region.xlsx',\n",
    "    'continents': 'Continent.xlsx',\n",
    "    'attraction_types': 'Type.xlsx',\n",
    "    'visit_modes': 'Mode.xlsx'\n",
    "}\n",
    "\n",
    "for name, filename in file_paths.items():\n",
    "    try:\n",
    "        df = pd.read_excel(correct_path + filename)\n",
    "        datasets[name] = df\n",
    "        print(f\"‚úì {name}: {df.shape[0]:,} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {name}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(datasets)} datasets successfully\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2: DATA QUALITY ASSESSMENT AND CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n2. DATA QUALITY ASSESSMENT AND CLEANING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def clean_dataset(df, dataset_name, critical_columns=None):\n",
    "    \"\"\"Comprehensive data cleaning function\"\"\"\n",
    "    print(f\"\\nüìä CLEANING {dataset_name.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    original_shape = df.shape\n",
    "    cleaning_log = []\n",
    "    \n",
    "    # 1. Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        df = df.drop_duplicates()\n",
    "        cleaning_log.append(f\"Removed {duplicates} duplicate rows\")\n",
    "    \n",
    "    # 2. Handle missing values\n",
    "    missing_before = df.isnull().sum().sum()\n",
    "    if missing_before > 0:\n",
    "        print(f\"Missing values found:\")\n",
    "        missing_summary = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "        for col, count in missing_summary.items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"  {col}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Different strategies for different column types\n",
    "        for col in df.columns:\n",
    "            if df[col].isnull().any():\n",
    "                if df[col].dtype in ['int64', 'float64']:\n",
    "                    # Numerical columns - fill with median\n",
    "                    df[col].fillna(df[col].median(), inplace=True)\n",
    "                    cleaning_log.append(f\"Filled {col} missing values with median\")\n",
    "                elif df[col].dtype == 'object':\n",
    "                    # Categorical columns - fill with mode or 'Unknown'\n",
    "                    mode_val = df[col].mode()\n",
    "                    if len(mode_val) > 0:\n",
    "                        df[col].fillna(mode_val.iloc[0], inplace=True)\n",
    "                        cleaning_log.append(f\"Filled {col} missing values with mode\")\n",
    "                    else:\n",
    "                        df[col].fillna('Unknown', inplace=True)\n",
    "                        cleaning_log.append(f\"Filled {col} missing values with 'Unknown'\")\n",
    "    \n",
    "    # 3. Data type optimization\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'int64':\n",
    "            # Check if can be converted to smaller int type\n",
    "            col_min, col_max = df[col].min(), df[col].max()\n",
    "            if col_min >= 0:\n",
    "                if col_max < 255:\n",
    "                    df[col] = df[col].astype('uint8')\n",
    "                elif col_max < 65535:\n",
    "                    df[col] = df[col].astype('uint16')\n",
    "                elif col_max < 4294967295:\n",
    "                    df[col] = df[col].astype('uint32')\n",
    "            else:\n",
    "                if col_min >= -128 and col_max <= 127:\n",
    "                    df[col] = df[col].astype('int8')\n",
    "                elif col_min >= -32768 and col_max <= 32767:\n",
    "                    df[col] = df[col].astype('int16')\n",
    "    \n",
    "    # 4. Remove invalid entries if critical columns specified\n",
    "    if critical_columns:\n",
    "        for col, (min_val, max_val) in critical_columns.items():\n",
    "            if col in df.columns:\n",
    "                before_count = len(df)\n",
    "                df = df[(df[col] >= min_val) & (df[col] <= max_val)]\n",
    "                removed = before_count - len(df)\n",
    "                if removed > 0:\n",
    "                    cleaning_log.append(f\"Removed {removed} rows with invalid {col} values\")\n",
    "    \n",
    "    # Print cleaning summary\n",
    "    final_shape = df.shape\n",
    "    print(f\"Shape change: {original_shape} ‚Üí {final_shape}\")\n",
    "    if cleaning_log:\n",
    "        print(\"Cleaning actions taken:\")\n",
    "        for action in cleaning_log:\n",
    "            print(f\"  ‚Ä¢ {action}\")\n",
    "    else:\n",
    "        print(\"No cleaning required - data was already clean!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean each dataset\n",
    "if 'transactions' in datasets:\n",
    "    # Define validation rules for transactions\n",
    "    transaction_rules = {\n",
    "        'Rating': (1, 5),\n",
    "        'VisitMonth': (1, 12),\n",
    "        'VisitYear': (2018, 2024)\n",
    "    }\n",
    "    datasets['transactions'] = clean_dataset(datasets['transactions'], 'transactions', transaction_rules)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if name != 'transactions':  # Already cleaned above\n",
    "        datasets[name] = clean_dataset(datasets[name], name)\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3: DATA INTEGRATION AND MASTER DATASET CREATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n3. DATA INTEGRATION AND MASTER DATASET CREATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if len(datasets) >= 4:  # Need at least main datasets\n",
    "    print(\"Creating integrated master dataset...\")\n",
    "    \n",
    "    # Start with transactions as base\n",
    "    if 'transactions' in datasets:\n",
    "        master_df = datasets['transactions'].copy()\n",
    "        print(f\"Starting with transactions: {len(master_df):,} records\")\n",
    "        \n",
    "        # Join with users (geographic information)\n",
    "        if 'users' in datasets:\n",
    "            master_df = master_df.merge(datasets['users'], on='UserId', how='left')\n",
    "            print(f\"After joining users: {len(master_df):,} records\")\n",
    "        \n",
    "        # Join with items (attraction information)\n",
    "        if 'items' in datasets:\n",
    "            master_df = master_df.merge(datasets['items'], on='AttractionId', how='left')\n",
    "            print(f\"After joining attractions: {len(master_df):,} records\")\n",
    "        \n",
    "        # Join with reference tables\n",
    "        reference_joins = [\n",
    "            ('cities', 'CityId', 'CityId'),\n",
    "            ('countries', 'CountryId', 'CountryId'),\n",
    "            ('regions', 'RegionId', 'RegionId'),\n",
    "            ('continents', 'ContinentId', 'ContinentId'),\n",
    "            ('attraction_types', 'AttractionTypeId', 'AttractionTypeId'),\n",
    "            ('visit_modes', 'VisitModeId', 'VisitMode')\n",
    "        ]\n",
    "        \n",
    "        for ref_name, join_col, master_col in reference_joins:\n",
    "            if ref_name in datasets and join_col in datasets[ref_name].columns:\n",
    "                if master_col in master_df.columns:\n",
    "                    before_cols = len(master_df.columns)\n",
    "                    master_df = master_df.merge(\n",
    "                        datasets[ref_name], \n",
    "                        left_on=master_col, \n",
    "                        right_on=join_col, \n",
    "                        how='left',\n",
    "                        suffixes=('', f'_{ref_name}')\n",
    "                    )\n",
    "                    after_cols = len(master_df.columns)\n",
    "                    print(f\"Joined {ref_name}: +{after_cols - before_cols} columns\")\n",
    "        \n",
    "        print(f\"\\nFinal master dataset: {master_df.shape[0]:,} rows √ó {master_df.shape[1]} columns\")\n",
    "        print(f\"Columns: {list(master_df.columns)}\")\n",
    "        \n",
    "        # Check join quality\n",
    "        print(f\"\\nJoin Quality Assessment:\")\n",
    "        join_quality = {}\n",
    "        if 'UserId' in master_df.columns:\n",
    "            missing_users = master_df['ContinentId'].isnull().sum()\n",
    "            join_quality['User Geography'] = f\"{((len(master_df) - missing_users) / len(master_df)) * 100:.1f}%\"\n",
    "        \n",
    "        if 'AttractionId' in master_df.columns:\n",
    "            missing_attractions = master_df['Attraction'].isnull().sum()\n",
    "            join_quality['Attraction Info'] = f\"{((len(master_df) - missing_attractions) / len(master_df)) * 100:.1f}%\"\n",
    "        \n",
    "        for metric, score in join_quality.items():\n",
    "            print(f\"  {metric}: {score} join success rate\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Cannot create master dataset without transactions data\")\n",
    "        master_df = None\n",
    "else:\n",
    "    print(\"‚ùå Insufficient datasets loaded for integration\")\n",
    "    master_df = None\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4: FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "if master_df is not None:\n",
    "    print(\"\\n4. FEATURE ENGINEERING\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"Creating derived features...\")\n",
    "    \n",
    "    # Temporal features\n",
    "    if 'VisitMonth' in master_df.columns:\n",
    "        # Season mapping\n",
    "        season_map = {12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "                     3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "                     6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "                     9: 'Fall', 10: 'Fall', 11: 'Fall'}\n",
    "        master_df['Season'] = master_df['VisitMonth'].map(season_map)\n",
    "        \n",
    "        # Month categories\n",
    "        master_df['MonthCategory'] = master_df['VisitMonth'].apply(\n",
    "            lambda x: 'Peak' if x in [6, 7, 8, 12] else 'Regular'\n",
    "        )\n",
    "        print(\"‚úì Created temporal features (Season, MonthCategory)\")\n",
    "    \n",
    "    # User-level aggregated features\n",
    "    if 'UserId' in master_df.columns:\n",
    "        user_stats = master_df.groupby('UserId').agg({\n",
    "            'Rating': ['count', 'mean', 'std'],\n",
    "            'AttractionId': 'nunique'\n",
    "        }).round(2)\n",
    "        \n",
    "        user_stats.columns = ['UserVisitCount', 'UserAvgRating', 'UserRatingStd', 'UserUniqueAttractions']\n",
    "        user_stats['UserRatingStd'] = user_stats['UserRatingStd'].fillna(0)  # Single visits have no std\n",
    "        \n",
    "        master_df = master_df.merge(user_stats, on='UserId', how='left')\n",
    "        print(\"‚úì Created user aggregation features\")\n",
    "    \n",
    "    # Attraction-level features\n",
    "    if 'AttractionId' in master_df.columns:\n",
    "        attraction_stats = master_df.groupby('AttractionId').agg({\n",
    "            'Rating': ['count', 'mean', 'std'],\n",
    "            'UserId': 'nunique'\n",
    "        }).round(2)\n",
    "        \n",
    "        attraction_stats.columns = ['AttractionPopularity', 'AttractionAvgRating', 'AttractionRatingStd', 'AttractionUniqueVisitors']\n",
    "        attraction_stats['AttractionRatingStd'] = attraction_stats['AttractionRatingStd'].fillna(0)\n",
    "        \n",
    "        master_df = master_df.merge(attraction_stats, on='AttractionId', how='left')\n",
    "        print(\"‚úì Created attraction aggregation features\")\n",
    "    \n",
    "    # Geographic diversity features\n",
    "    if all(col in master_df.columns for col in ['UserId', 'CountryId']):\n",
    "        user_country_diversity = master_df.groupby('UserId')['CountryId'].nunique()\n",
    "        user_country_diversity.name = 'UserCountryDiversity'\n",
    "        master_df = master_df.merge(user_country_diversity, on='UserId', how='left')\n",
    "        print(\"‚úì Created geographic diversity features\")\n",
    "    \n",
    "    # Rating deviation features\n",
    "    if 'Rating' in master_df.columns and 'UserAvgRating' in master_df.columns:\n",
    "        master_df['RatingDeviation'] = master_df['Rating'] - master_df['UserAvgRating']\n",
    "        print(\"‚úì Created rating deviation features\")\n",
    "    \n",
    "    # Visit mode preferences\n",
    "    if all(col in master_df.columns for col in ['UserId', 'VisitMode']):\n",
    "        user_mode_counts = master_df.groupby(['UserId', 'VisitMode']).size().unstack(fill_value=0)\n",
    "        user_mode_counts.columns = [f'UserMode_{col}' for col in user_mode_counts.columns]\n",
    "        master_df = master_df.merge(user_mode_counts, on='UserId', how='left')\n",
    "        print(\"‚úì Created visit mode preference features\")\n",
    "    \n",
    "    print(f\"\\nFeature engineering complete. Dataset now has {master_df.shape[1]} columns\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5: CATEGORICAL ENCODING\n",
    "# =============================================================================\n",
    "\n",
    "if master_df is not None:\n",
    "    print(\"\\n5. CATEGORICAL ENCODING\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_columns = master_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Remove columns that are just names/addresses (not for modeling)\n",
    "    exclude_columns = ['Attraction', 'AttractionAddress', 'CityName']\n",
    "    categorical_columns = [col for col in categorical_columns if col not in exclude_columns]\n",
    "    \n",
    "    print(f\"Categorical columns to encode: {categorical_columns}\")\n",
    "    \n",
    "    # Label encoding for ordinal variables\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    label_encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        if col in master_df.columns:\n",
    "            le = LabelEncoder()\n",
    "            master_df[f'{col}_Encoded'] = le.fit_transform(master_df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "            print(f\"‚úì Label encoded {col}: {len(le.classes_)} unique values\")\n",
    "    \n",
    "    # One-hot encoding for key categorical variables (limit to prevent explosion)\n",
    "    onehot_columns = ['Season', 'MonthCategory']\n",
    "    onehot_columns = [col for col in onehot_columns if col in master_df.columns]\n",
    "    \n",
    "    if onehot_columns:\n",
    "        master_df_encoded = pd.get_dummies(master_df, columns=onehot_columns, prefix=onehot_columns)\n",
    "        print(f\"‚úì One-hot encoded: {onehot_columns}\")\n",
    "        print(f\"Dataset shape after encoding: {master_df_encoded.shape}\")\n",
    "    else:\n",
    "        master_df_encoded = master_df.copy()\n",
    "    \n",
    "    # Save encoding information for later use\n",
    "    encoding_info = {\n",
    "        'label_encoders': label_encoders,\n",
    "        'categorical_columns': categorical_columns,\n",
    "        'onehot_columns': onehot_columns\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 6: DATA QUALITY VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "if master_df is not None:\n",
    "    print(\"\\n6. FINAL DATA QUALITY VALIDATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_vals = master_df_encoded.isnull().sum()\n",
    "    missing_cols = missing_vals[missing_vals > 0]\n",
    "    \n",
    "    if len(missing_cols) > 0:\n",
    "        print(\"Remaining missing values:\")\n",
    "        for col, count in missing_cols.items():\n",
    "            pct = (count / len(master_df_encoded)) * 100\n",
    "            print(f\"  {col}: {count} ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(\"‚úì No missing values in final dataset\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(f\"\\nData type summary:\")\n",
    "    dtype_counts = master_df_encoded.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # Memory usage\n",
    "    memory_mb = master_df_encoded.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"\\nMemory usage: {memory_mb:.2f} MB\")\n",
    "    \n",
    "    # Basic statistics for key columns\n",
    "    if 'Rating' in master_df_encoded.columns:\n",
    "        print(f\"\\nKey statistics:\")\n",
    "        print(f\"  Rating range: {master_df_encoded['Rating'].min()} - {master_df_encoded['Rating'].max()}\")\n",
    "        print(f\"  Average rating: {master_df_encoded['Rating'].mean():.2f}\")\n",
    "        print(f\"  Total records: {len(master_df_encoded):,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 7: DATA SPLITTING FOR MODELING\n",
    "# =============================================================================\n",
    "\n",
    "if master_df is not None:\n",
    "    print(\"\\n7. DATA SPLITTING FOR MODELING\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Define feature columns (exclude IDs and target variables)\n",
    "    exclude_cols = ['TransactionId', 'UserId', 'AttractionId', 'Attraction', 'AttractionAddress']\n",
    "    \n",
    "    # Features for regression (predict Rating)\n",
    "    regression_features = [col for col in master_df_encoded.columns \n",
    "                          if col not in exclude_cols + ['Rating']]\n",
    "    \n",
    "    # Features for classification (predict VisitMode)\n",
    "    classification_features = [col for col in master_df_encoded.columns \n",
    "                              if col not in exclude_cols + ['VisitMode', 'VisitModeId']]\n",
    "    \n",
    "    if 'Rating' in master_df_encoded.columns:\n",
    "        # Regression data split\n",
    "        X_reg = master_df_encoded[regression_features]\n",
    "        y_reg = master_df_encoded['Rating']\n",
    "        \n",
    "        X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "            X_reg, y_reg, test_size=0.2, random_state=42, stratify=y_reg\n",
    "        )\n",
    "        \n",
    "        print(f\"Regression split:\")\n",
    "        print(f\"  Training: {X_reg_train.shape}\")\n",
    "        print(f\"  Testing: {X_reg_test.shape}\")\n",
    "        print(f\"  Features: {len(regression_features)}\")\n",
    "    \n",
    "    if 'VisitMode' in master_df_encoded.columns:\n",
    "        # Classification data split\n",
    "        X_class = master_df_encoded[classification_features]\n",
    "        y_class = master_df_encoded['VisitMode']\n",
    "        \n",
    "        X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(\n",
    "            X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    "        )\n",
    "        \n",
    "        print(f\"Classification split:\")\n",
    "        print(f\"  Training: {X_class_train.shape}\")\n",
    "        print(f\"  Testing: {X_class_test.shape}\")\n",
    "        print(f\"  Features: {len(classification_features)}\")\n",
    "    \n",
    "    # Recommendation data (user-item matrix)\n",
    "    if all(col in master_df_encoded.columns for col in ['UserId', 'AttractionId', 'Rating']):\n",
    "        recommendation_data = master_df_encoded[['UserId', 'AttractionId', 'Rating']].copy()\n",
    "        \n",
    "        # Create user-item matrix\n",
    "        user_item_matrix = recommendation_data.pivot_table(\n",
    "            index='UserId', \n",
    "            columns='AttractionId', \n",
    "            values='Rating', \n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        print(f\"Recommendation matrix: {user_item_matrix.shape}\")\n",
    "        print(f\"  Sparsity: {(user_item_matrix == 0).sum().sum() / user_item_matrix.size * 100:.1f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 8: SAVE PROCESSED DATA\n",
    "# =============================================================================\n",
    "\n",
    "if master_df is not None:\n",
    "    print(\"\\n8. SAVING PROCESSED DATA\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create processed data directory\n",
    "    processed_dir = correct_path.replace('raw', 'processed')\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "    # Save master dataset\n",
    "    master_df_encoded.to_csv(processed_dir + 'master_dataset.csv', index=False)\n",
    "    print(f\"‚úì Saved master dataset: {processed_dir}master_dataset.csv\")\n",
    "    \n",
    "    # Save splits for modeling\n",
    "    if 'Rating' in master_df_encoded.columns:\n",
    "        pd.concat([X_reg_train, y_reg_train], axis=1).to_csv(\n",
    "            processed_dir + 'train_regression.csv', index=False)\n",
    "        pd.concat([X_reg_test, y_reg_test], axis=1).to_csv(\n",
    "            processed_dir + 'test_regression.csv', index=False)\n",
    "        print(\"‚úì Saved regression train/test splits\")\n",
    "    \n",
    "    if 'VisitMode' in master_df_encoded.columns:\n",
    "        pd.concat([X_class_train, y_class_train], axis=1).to_csv(\n",
    "            processed_dir + 'train_classification.csv', index=False)\n",
    "        pd.concat([X_class_test, y_class_test], axis=1).to_csv(\n",
    "            processed_dir + 'test_classification.csv', index=False)\n",
    "        print(\"‚úì Saved classification train/test splits\")\n",
    "    \n",
    "    if 'user_item_matrix' in locals():\n",
    "        user_item_matrix.to_csv(processed_dir + 'user_item_matrix.csv')\n",
    "        recommendation_data.to_csv(processed_dir + 'recommendation_data.csv', index=False)\n",
    "        print(\"‚úì Saved recommendation data\")\n",
    "    \n",
    "    # Save metadata\n",
    "    import json\n",
    "    metadata = {\n",
    "        'processing_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'original_shape': list(datasets['transactions'].shape) if 'transactions' in datasets else None,\n",
    "        'final_shape': list(master_df_encoded.shape),\n",
    "        'total_features': len(regression_features) if 'regression_features' in locals() else None,\n",
    "        'categorical_columns': categorical_columns if 'categorical_columns' in locals() else [],\n",
    "        'data_quality_score': 'Good' if len(missing_cols) == 0 else 'Moderate'\n",
    "    }\n",
    "    \n",
    "    with open(processed_dir + 'processing_metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(\"‚úì Saved processing metadata\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 9: SUMMARY AND NEXT STEPS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n9. DATA PREPROCESSING SUMMARY\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if master_df is not None:\n",
    "    print(\"‚úÖ DATA PREPROCESSING COMPLETED SUCCESSFULLY\")\n",
    "    print(f\"   Original data: {len(datasets['transactions']):,} transactions\" if 'transactions' in datasets else \"\")\n",
    "    print(f\"   Final dataset: {master_df_encoded.shape[0]:,} rows √ó {master_df_encoded.shape[1]} columns\")\n",
    "    print(f\"   Memory usage: {master_df_encoded.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"   Data quality: {'‚úì Excellent' if len(missing_cols) == 0 else '‚ö† Good'}\")\n",
    "    \n",
    "    print(f\"\\nüìã Ready for modeling:\")\n",
    "    print(f\"   ‚Ä¢ Regression (Rating prediction): {len(regression_features) if 'regression_features' in locals() else 0} features\")\n",
    "    print(f\"   ‚Ä¢ Classification (Visit mode): {len(classification_features) if 'classification_features' in locals() else 0} features\")\n",
    "    print(f\"   ‚Ä¢ Recommendation system: User-item matrix ready\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Processed files saved to: {processed_dir}\")\n",
    "    print(f\"   ‚Ä¢ master_dataset.csv\")\n",
    "    print(f\"   ‚Ä¢ train/test splits for regression and classification\")\n",
    "    print(f\"   ‚Ä¢ recommendation data and user-item matrix\")\n",
    "    print(f\"   ‚Ä¢ processing metadata\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå DATA PREPROCESSING FAILED\")\n",
    "    print(\"Please check data loading and try again\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ NEXT STEP: Feature Engineering and Model Building\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tourism_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
